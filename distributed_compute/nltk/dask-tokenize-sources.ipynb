{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This script needs to be run on a properly configured Dask cluster. It looks for files on a mounted Samba share at /picluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, re, keyring, nltk\n",
    "from dask.distributed import Client, get_worker\n",
    "import dask.bag as db\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define the server details\n",
    "server_name = '192.168.1.200'\n",
    "shared_dir = 'TechnicalShare'\n",
    "working_dir = 'datascience/found-infrastructure'\n",
    "\n",
    "# Get the username and password from keyring\n",
    "username = 'cluster_worker'\n",
    "password = keyring.get_password('cluster', username)\n",
    "\n",
    "# Create an SMB connection and test it\n",
    "command = f\"smbclient //{server_name}/{shared_dir} -U {username}%{password} -c 'exit'\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print('Connection to file share successful.\\n')\n",
    "else:\n",
    "    print('Failed to connect to file share.')\n",
    "\n",
    "# Traverse to the working directory, print its filepath, and list its contents\n",
    "command = f\"smbclient //{server_name}/{shared_dir} -U {username}%{password} -c 'cd {working_dir}; pwd; ls'\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print('Traversed to working directory successfully.\\n')\n",
    "    # Split the output into lines\n",
    "    lines = result.stdout.split('\\n')\n",
    "    # Find and print the line that contains the present working directory\n",
    "    for line in lines:\n",
    "        if 'Current directory is' in line:\n",
    "            print(line)\n",
    "            break\n",
    "    print('\\nDirectory contents incude:\\n')\n",
    "    # Filter out hidden files and specified files/directories\n",
    "    exclude_items = ['.', '__pycache__']\n",
    "    lines = [line.strip() for line in lines if not any(item in line.strip() for item in exclude_items)]\n",
    "    print('\\n'.join(lines))\n",
    "else:\n",
    "    print('Failed to traverse to working directory.')\n",
    "\n",
    "\n",
    "# Start parallel computing with Dask. \n",
    "# Note that Dask needs the absolute file path to source files, not the network file path used above. \n",
    "# The location of the mounted Samba share needs to be the same on each node.\n",
    "\n",
    "# Connect to Dask distributed cluster\n",
    "from dask.distributed import Client\n",
    "client = Client('tcp://192.168.1.200:8786') \n",
    "\n",
    "# Use Dask's read_text to get the contents of the directory. \n",
    "import dask.bag as db\n",
    "contents = db.read_text('/picluster/datascience/found-infrastructure/sources/source_files/*').compute()\n",
    "\n",
    "# Load your text files\n",
    "b = db.read_text('/picluster/datascience/found-infrastructure/sources/source_files/*.txt')\n",
    "\n",
    "# Set the mode ('complete' or 'novels')\n",
    "mode = 'novels'  # Change this to 'novels' if needed\n",
    "\n",
    "# Define your regular expression for the title markers\n",
    "title_marker = re.compile(r'\\[[A-Z\\']+\\]')\n",
    "\n",
    "def process_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # If analyzing the entire corpus, you can remove the title markers\n",
    "    if mode == 'complete':\n",
    "        tokens = [token for token in tokens if not title_marker.match(token)]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # If analyzing individual novels, you can split the tokens list into sublists\n",
    "    elif mode == 'novels':\n",
    "        novels = []\n",
    "        novel = []\n",
    "        for token in tokens:\n",
    "            if title_marker.match(token):\n",
    "                if novel:  # if the novel list is not empty, add it to novels\n",
    "                    novels.append(novel)\n",
    "                    novel = []  # start a new novel\n",
    "            else:\n",
    "                novel.append(token)\n",
    "\n",
    "        if novel:  # add the last novel\n",
    "            novels.append(novel)\n",
    "\n",
    "        return '\\n'.join([' '.join(novel_tokens) for novel_tokens in novels])\n",
    "\n",
    "# Apply the function to each text file in parallel\n",
    "results = b.map(process_text).compute()\n",
    "\n",
    "# Save results \n",
    "with open('../output/tokenized_merged_titles.txt', 'w') as output_file:\n",
    "    output_file.write('\\n'.join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the file has been tokenized. \n",
    "\n",
    "def is_tokenized(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        # If the first line contains spaces, it's likely the file has been tokenized\n",
    "        return ' ' in first_line\n",
    "\n",
    "print(is_tokenized('/picluster/datascience/trollope-gutenberg/distributed_compute/nltk/nltk-output/tokenized_merged_novels.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
