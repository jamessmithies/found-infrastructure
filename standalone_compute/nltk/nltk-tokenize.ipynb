{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load your text file\n",
    "with open('../output/merged_titles.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Set the mode ('complete' or 'novels')\n",
    "mode = 'novels'  # Change this to 'novels' if needed\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Define your regular expression for the title markers\n",
    "title_marker = re.compile(r'\\[[A-Z\\']+\\]')\n",
    "\n",
    "# If analyzing the entire corpus, you can remove the title markers\n",
    "if mode == 'complete':\n",
    "    tokens = [token for token in tokens if not title_marker.match(token)]\n",
    "    # Save tokenized text to 'tokenized_complete.txt'\n",
    "    with open('../output/tokenized_merged_titles.txt', 'w') as output_file:\n",
    "        output_file.write(' '.join(tokens))\n",
    "    # Save a smaller file, for test purposes\n",
    "    with open('../output/tokenized_merged_titles.txt', 'r') as input_file, open('../output/tokenized_merged_titles_test.txt', 'w') as output_file:\n",
    "        content = input_file.read(10000)\n",
    "        output_file.write(content)\n",
    "\n",
    "\n",
    "# If analyzing individual novels, you can split the tokens list into sublists\n",
    "elif mode == 'novels':\n",
    "    novels = []\n",
    "    novel = []\n",
    "    for token in tokens:\n",
    "        if title_marker.match(token):\n",
    "            if novel:  # if the novel list is not empty, add it to novels\n",
    "                novels.append(novel)\n",
    "                novel = []  # start a new novel\n",
    "        else:\n",
    "            novel.append(token)\n",
    "    \n",
    "    if novel:  # add the last novel\n",
    "        novels.append(novel)\n",
    "    \n",
    "    # Save tokenized novels to 'tokenized_novels.txt'\n",
    "    with open('../output/tokenized_merged_titles.txt', 'w') as output_file:\n",
    "        for novel_tokens in novels:\n",
    "            output_file.write(' '.join(novel_tokens) + '\\n')\n",
    "\n",
    "    # Save a smaller file, for test purposes\n",
    "    with open('../output/tokenized_merged_titles.txt', 'r') as input_file, open('../output/tokenized_merged_titles_test.txt', 'w') as output_file:\n",
    "        content = input_file.read(10000)\n",
    "        output_file.write(content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
